{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Audio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device (PyTorch):\", device)\n",
    "\n",
    "# Check device for TensorFlow\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU is available (TensorFlow)\")\n",
    "else:\n",
    "    print(\"GPU is not available (TensorFlow)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory for the CREMA-D dataset\n",
    "Crema = \"../datasets/CREMA_D\"\n",
    "crema_directory_list = os.listdir(Crema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # Correctly adding the separator\n",
    "    file_path.append(os.path.join(Crema, file))\n",
    "    # Storing file emotions based on naming convention\n",
    "    part = file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for emotions of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "# DataFrame for path of files\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the count of emotions\n",
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(Crema_df.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title('Waveplot for {} emotion'.format(e), size=15)\n",
    "    librosa.display.waveshow(data, sr=sr)  # Use waveshow instead of waveplot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(data, sr, e):\n",
    "    X = librosa.stft(data)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title('Spectrogram for {} emotion'.format(e), size=15)\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the emotion you're interested in\n",
    "emotion = 'angry'\n",
    "# Get the path of the first audio file for the specified emotion\n",
    "path = np.array(Crema_df.Path[Crema_df.Emotions == emotion])[0]\n",
    "\n",
    "# Check if the file exists before trying to load it\n",
    "if os.path.isfile(path):\n",
    "    data, sampling_rate = librosa.load(path)\n",
    "    create_waveplot(data, sampling_rate, emotion)\n",
    "    create_spectrogram(data, sampling_rate, emotion)\n",
    "    display(Audio(path))\n",
    "else:\n",
    "    print(f\"File not found: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFCC Extraction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'disgust':0,'happy':1,'sad':2,'neutral':3,'fear':4,'angry':5}\n",
    "Crema_df.replace({'Emotions':labels},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for MFCC extraction\n",
    "num_mfcc = 13\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "data = {\n",
    "    \"labels\": [],\n",
    "    \"mfcc\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFCC Extraction\n",
    "labels = {'disgust': 0, 'happy': 1, 'sad': 2, 'neutral': 3, 'fear': 4, 'angry': 5}\n",
    "Crema_df.replace({'Emotions': labels}, inplace=True)\n",
    "\n",
    "# Set parameters for MFCC extraction\n",
    "num_mfcc = 13\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "data = {\n",
    "    \"labels\": [],\n",
    "    \"mfcc\": []\n",
    "}\n",
    "\n",
    "# Extract MFCC for each audio file\n",
    "for i in range(len(Crema_df)):\n",
    "    data['labels'].append(Crema_df.iloc[i, 0])\n",
    "    \n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(Crema_df.iloc[i, 1], sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Extract MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    mfcc = mfcc.T  # Transpose to match shape\n",
    "    \n",
    "    # Store MFCCs\n",
    "    data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "    \n",
    "    # Print progress\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Processed {i} files\")\n",
    "\n",
    "# Convert to DataFrame for further processing if needed\n",
    "mfcc_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the MFCC DataFrame\n",
    "print(mfcc_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding MFCC to make them of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum length of MFCC arrays\n",
    "max_length = max([len(mfcc) for mfcc in data['mfcc']])\n",
    "\n",
    "# Pad the MFCC arrays\n",
    "X = pad_sequences(data['mfcc'], maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "# Convert labels to a NumPy array\n",
    "y = np.asarray(data[\"labels\"])\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,y_train.shape,X_validation.shape,y_validation.shape,X_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(LSTM(64))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "input_shape = (None,13)\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# compile model\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimiser,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Speech-Emotion-Recognition-Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, num_mfcc=13, n_fft=2048, hop_length=512, sample_rate=22050):\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(file_path, sr=sample_rate)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # Transpose the MFCC to match the input shape (time steps, MFCC coefficients)\n",
    "    mfcc = mfcc.T\n",
    "    \n",
    "    # Pad the MFCC features to match the input shape of the model\n",
    "    mfcc = tf.keras.preprocessing.sequence.pad_sequences([mfcc], maxlen=X.shape[1], padding='post')\n",
    "    \n",
    "    return np.array(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, file_path):\n",
    "    # Preprocess the audio file\n",
    "    mfcc = preprocess_audio(file_path)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(mfcc)\n",
    "    \n",
    "    # Get the index with the highest probability\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "    \n",
    "    return predicted_index[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from label to emotion\n",
    "labels_to_emotions = {0: 'disgust', 1: 'happy', 2: 'sad', 3: 'neutral', 4: 'fear', 5: 'angry'}\n",
    "\n",
    "# Example: Path to your pre-recorded audio file\n",
    "file_path = \"C:\\\\Users\\\\dell\\\\Desktop\\\\graduation project\\\\AI04-Grad\\\\datasets\\\\CREMA-D\\\\AudioWAV\\\\1001_ITH_FEA_XX.wav\"\n",
    "# Predict the emotion\n",
    "predicted_label = predict_emotion(model, file_path)\n",
    "predicted_emotion = labels_to_emotions[predicted_label]\n",
    "\n",
    "print(\"Predicted Emotion: \", predicted_emotion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
